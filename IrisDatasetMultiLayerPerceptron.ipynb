{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Tatiana DeRouen\n",
    "##Neural Network on the Iris Dataset Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "#Iris Dataset\n",
    "iris = datasets.load_iris()\n",
    "X_true = iris.data\n",
    "y_true = iris.target\n",
    "\n",
    "#Split the dataset\n",
    "from sklearn.cross_validation import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_true, y_true)\n",
    "\n",
    "#change the X and y values to the testing dataset\n",
    "X = X_test\n",
    "y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_size = len(X) #this will be the size of the dataset (could be test or training)\n",
    "numInput = 4 # number of inputs \n",
    "numOutput = 3 # number of outputs  \n",
    "\n",
    "lr = 0.01 # learning rate for gradient descent\n",
    "reg = 0.01 # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict an output (0,1, or 2). 0, 1, 2 correspond with the iris flower type\n",
    "def predictIris(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation\n",
    "    dotValue = x.dot(W1) + b1\n",
    "    a = np.tanh(dotValue)\n",
    "    dotValue2 = a.dot(W2) + b2\n",
    "    scores = np.exp(dotValue2)\n",
    "    prob = scores / np.sum(scores, axis=1, keepdims=True)\n",
    "    return np.argmax(prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This funciton uses the number of hidden layers and number of passes to learn the neural network\n",
    "# The parameters for the build_nn function are: nhi, passes, and print_loss\n",
    "# nhi is the number of hidden layers\n",
    "# passes are the how many times it goes through the dataset\n",
    "# when print loss is used, it prints the calculated loss after every 1000 iterations\n",
    "def build_NN(nhi, passes=20000, print_loss=False):\n",
    "    \n",
    "    # Randomizing the weights and biases\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(numInput, nhi) / np.sqrt(numInput)\n",
    "    b1 = np.zeros((1, nhi))\n",
    "    W2 = np.random.randn(nhi, numOutput) / np.sqrt(nhi)\n",
    "    b2 = np.zeros((1, numOutput))\n",
    "\n",
    "    # model that will be returned\n",
    "    NN = {}\n",
    "    \n",
    "    # Gradient descent for each pass\n",
    "    for i in xrange(0, passes):\n",
    "\n",
    "        # Forward propagation\n",
    "        dotValue = X.dot(W1) + b1\n",
    "        a = np.tanh(dotValue)\n",
    "        dotValue2 = a.dot(W2) + b2\n",
    "        scores = np.exp(dotValue2)\n",
    "        probs = scores / np.sum(scores, axis=1, keepdims=True)\n",
    "       \n",
    "        # Backpropagation\n",
    "        delta3 = probs\n",
    "        delta3[range(set_size), y] -= 1\n",
    "        dW2 = (a.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a, 2))\n",
    "        dW1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Regularization terms \n",
    "        dW2 += reg * W2\n",
    "        dW1 += reg * W1\n",
    "\n",
    "        # Update our weights and biases\n",
    "        W1 += -lr * dW1\n",
    "        b1 += -lr * db1\n",
    "        W2 += -lr * dW2\n",
    "        b2 += -lr * db2\n",
    "        \n",
    "        # New model parameters\n",
    "        NN = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        \n",
    "    \n",
    "    return NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b2': array([[-0.45105344,  0.94541234, -0.49435891]]), 'b1': array([[ 0.12685271, -4.56791704,  0.146556  ]]), 'W1': array([[  0.42971816,  -2.72514369,   0.31342158],\n",
      "       [  0.20870173,  -5.54368959,  -1.47040227],\n",
      "       [  0.5275936 ,   4.11130626,   0.43008883],\n",
      "       [  0.22073016,  10.12852074,   1.14828776]]), 'W2': array([[ 0.28953703,  0.31528748, -0.50118171],\n",
      "       [-1.54477767, -0.97925518,  2.65080303],\n",
      "       [-5.14495687,  1.76277187,  3.14045542]])}\n",
      "\n",
      "Predicted outputs for the test dataset: \n",
      "[2 0 0 1 2 1 0 2 2 0 0 2 0 1 2 1 0 1 2 2 1 2 1 2 0 2 1 2 1 0 0 2 1 1 0 0 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "# Neural Network with a 3 hidden layers\n",
    "nn = build_NN(3, print_loss=True)\n",
    "\n",
    "print(nn)\n",
    "print(\"\\nPredicted outputs for the test dataset: \")\n",
    "\n",
    "print(predictIris(nn, X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
